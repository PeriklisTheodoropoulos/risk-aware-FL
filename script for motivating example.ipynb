{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import math\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import copy\n",
    "import ray\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import os, json , time \n",
    "from tqdm import trange\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#############################################################################\n",
    "################### Motivating example with 3 classes - 3 users #############\n",
    "\n",
    "# Hyperparameters\n",
    "# two choices for a. a=1. (risk-neutral), a=0.1 (risk-aware)\n",
    "cvar_alpha_list = [1.,0.1]\n",
    "# parameeter gamma\n",
    "gamma = 0.1\n",
    "# number of users\n",
    "K = 3\n",
    "# number of global rounds\n",
    "T = 4000\n",
    "# number of local epochs\n",
    "H = 10\n",
    "# users probabilities\n",
    "user_probs = [  0.5 , 0.4, 0.1 ]\n",
    "# choose how many users the RAM will allow to send to the server\n",
    "users_per_round = 1\n",
    "# learning rates for theta and t, respectively\n",
    "learning_rate , learning_rate_t = 0.001 , 0.001\n",
    "# beta parameter is used for visualizing purpose (not necessary for the algorithm)\n",
    "beta =0.9\n",
    "boundary_list = []\n",
    "t_final_list = []\n",
    "##########################################################################################################################################################\n",
    "##########################################################################################################################################################\n",
    "\n",
    "##########################################################################################################################################################\n",
    "##########################################################################################################################################################\n",
    "# Generate 2D-synthetic dataset\n",
    "# number of points for each class\n",
    "num_points = 1000\n",
    "X, y = make_blobs(n_samples=[num_points,num_points, num_points ], centers=[(1, 1),(2, 3),(2.3, 1.5)], cluster_std=[0.1,0.1, 0.1], n_features=1, shuffle=False, random_state=11)\n",
    "# number of classes\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)\n",
    "\n",
    "# Convert training and testing set into tebsors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32) # Convert to PyTorch tensor\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(X_test, dtype=torch.float32) # Convert to PyTorch tensor\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long) \n",
    "\n",
    "# Distribute the data to the users\n",
    "data_users = [X_train_tensor[y_train==i] for i in [0,1,2]]\n",
    "labels_users = [y_train_tensor[y_train==i] for i in [0,1,2]]\n",
    "\n",
    "# Plot the data points\n",
    "# plt.scatter(data_users[0][:,0], data_users[0][:,1],c=labels_users[0], cmap='BrBG',s=10, edgecolors='green')\n",
    "# plt.scatter(data_users[1][:,0], data_users[1][:,1],c=labels_users[1], cmap='BrBG',s=20, edgecolors='red')\n",
    "# plt.scatter(data_users[2][:,0], data_users[2][:,1],c=labels_users[2], cmap='BrBG',s=10, edgecolors='black')\n",
    "##########################################################################################################################################################\n",
    "##########################################################################################################################################################\n",
    "\n",
    "##########################################################################################################################################################\n",
    "##########################################################################################################################################################\n",
    "# Define the RAM model\n",
    "def RAM(user_weights,K,user_probs,users_per_round,t_list):\n",
    "    user_index = random.choices(range(K), weights=user_probs, k=users_per_round)[0]\n",
    "    return user_weights[user_index],user_index , t_list[user_index]\n",
    "##########################################################################################################################################################\n",
    "##########################################################################################################################################################\n",
    "\n",
    "##########################################################################################################################################################\n",
    "##########################################################################################################################################################\n",
    "# Define the model neural network\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(2, 64)\n",
    "        self.layer2 = nn.Linear(64, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.layer1.weight.data = torch.zeros([64,2])\n",
    "        self.layer2.weight.data = torch.zeros([3,64])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "##########################################################################################################################################################\n",
    "##########################################################################################################################################################\n",
    "\n",
    "\n",
    "ray.init(log_to_driver=False)\n",
    "# for cvar_alpha in cvar_alpha_list:\n",
    "@ray.remote(num_gpus=0.2,max_calls=0)\n",
    "# the  main function used in this example for both values of the parameter a.\n",
    "def Logistic_Regression_FL_RaM(cvar_alpha):\n",
    "\n",
    "    # Initialize the model and define the loss \n",
    "    global_model = MyModel()\n",
    "    \n",
    "    # Define the loss \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize the parameters theta and t\n",
    "    # Initialize the parameter theta \n",
    "    global_theta = global_model.state_dict()\n",
    "    user_weights = [ global_theta.copy() for _ in range(K) ] \n",
    "    t_list = [ torch.tensor(  0.  , requires_grad=True) for _ in range(K)  ]\n",
    "    \n",
    "    ##############################################\n",
    "    # The next two commants are important only for plotting (Not essential for the algorithm) \n",
    "    sampling_counts, a_0  = [0] * K , 1/K\n",
    "    a_i = [0] * K\n",
    "    ##############################################\n",
    "\n",
    "    t_list_track = []\n",
    "    test_loss_per_a = []\n",
    "    # start the training \n",
    "    for epoch in trange(1,T):\n",
    "        # pass the model in training mode\n",
    "        global_model.train()\n",
    "\n",
    "        # RAM send one user and its parameter (theta,t) to the Server\n",
    "        global_theta , user_index , global_t = RAM(user_weights,K,user_probs,users_per_round,t_list)\n",
    "        \n",
    "        #################################################################\n",
    "        #################################################################\n",
    "        # Update the appropriate lists in order to visualize the parameters \n",
    "        global_t = torch.tensor(global_t , requires_grad=True)\n",
    "        t_list_track.append(global_t.item())\n",
    "\n",
    "        ###########################################################################\n",
    "        #################### Counter of the user Availability #####################\n",
    "        sampling_counts[user_index] += 1\n",
    "        n_i = sampling_counts[user_index]\n",
    "        n = epoch\n",
    "        fraction_n_i_n = n_i / n if n != 0 else 1.0\n",
    "\n",
    "        for i in range(K):\n",
    "            a_i[i] = (1 - beta) * a_0 + beta * sampling_counts[i]/n\n",
    "\n",
    "        if round(sum(a_i)) != 1:\n",
    "            print(\"fraction_n_i_n\",fraction_n_i_n)\n",
    "            print(\"a_i:\",a_i)\n",
    "            sys.exit(\"List a_i has not sum equal to 1\")\n",
    "        ###########################################################################\n",
    "        ###########################################################################\n",
    "        global_model.load_state_dict(global_theta)\n",
    "\n",
    "        for user in range(K):\n",
    "            f_local = copy.deepcopy(global_model)\n",
    "            f_local.requires_grad_(True)\n",
    "\n",
    "            x_train_tensor = torch.tensor(data_users[user], dtype=torch.float32)\n",
    "            y_train_tensor = torch.tensor(labels_users[user], dtype=torch.long)\n",
    "            \n",
    "            # Define the data loader\n",
    "            train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "            batch_size =    100\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            # Set optimizers of theta and t\n",
    "            optimizer = optim.SGD(f_local.parameters(), lr=learning_rate)\n",
    "            global_t = torch.tensor(global_t,requires_grad=True)\n",
    "            optimizer_t = torch.optim.SGD([global_t], lr=learning_rate_t  )\n",
    "            \n",
    "            # For each global epoch\n",
    "            for _ in range(H):\n",
    "                # Loop over the data loader\n",
    "                for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "                    # Forward pass\n",
    "                    outputs = f_local(batch_x)\n",
    "\n",
    "                    # empirical local loss\n",
    "                    f_i = loss_fn(outputs, batch_y)\n",
    "                    # risk-aware local loss\n",
    "                    cvar_loss = (1-gamma) * global_t + ((1-gamma) * torch.relu(f_i - global_t))/cvar_alpha + gamma * f_i\n",
    "                    \n",
    "                    # optimize steps\n",
    "                    optimizer.zero_grad()\n",
    "                    optimizer_t.zero_grad()\n",
    "                    cvar_loss.backward(retain_graph = True)\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    optimizer_t.step()\n",
    "            # update the lists with users new theta's and t's\n",
    "            user_weights[user] = f_local.state_dict()\n",
    "            t_list[ user ] = global_t.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            global_model.load_state_dict(global_theta)\n",
    "            ##############################################\n",
    "            # pass the model in testing phase\n",
    "            global_model.eval() \n",
    "\n",
    "            y_pred = global_model(x_test_tensor)\n",
    "            test_loss = loss_fn(y_pred, y_test_tensor)\n",
    "            test_loss_per_a.append(test_loss)\n",
    "    print(f\" For a= {cvar_alpha}  the Sampling counts = {sampling_counts}, a_i: {a_i} \")    \n",
    "        \n",
    "    ############################################################################################\n",
    "    ############################################################################################\n",
    "    # Evaluate the model\n",
    "    # Plot the data and the decision boundary\n",
    "    t_final_list.append(t_list_track)\n",
    "    with torch.no_grad():\n",
    "        global_model.load_state_dict(global_theta)\n",
    "        ##############################################\n",
    "        # pass the model in testing phase\n",
    "        global_model.eval() \n",
    "        # Make predictions for each point in the grid\n",
    "        # Create a meshgrid of points in the feature space\n",
    "        x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "        y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "        X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "        # Predict the class labels for each point in the meshgrid\n",
    "        Z = np.argmax(global_model(torch.tensor(X_grid, dtype=torch.float32)).detach().numpy(), axis=1)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    return xx, yy, Z , t_list_track , test_loss_per_a\n",
    "ray.shutdown()\n",
    "\n",
    "results = ray.get( [ Logistic_Regression_FL_RaM.remote(cvar_alpha) for cvar_alpha in cvar_alpha_list ] )\n",
    "\n",
    "xx = [result[0] for result in results]\n",
    "yy = [result[1] for result in results]\n",
    "Z = [result[2] for result in results]\n",
    "t_list_track = [result[3] for result in results]\n",
    "test_loss_track = [result[4] for result in results]\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "### Plot the testing losses\n",
    "# Plot the testing loss for a = 1 and a = 0.1\n",
    "plt.figure()\n",
    "plt.plot(test_loss_track[0],color='green', label=f'Test loss for a= {cvar_alpha_list[0]}') # cvar_alpha_list[0] = a = 1\n",
    "plt.plot(test_loss_track[1],color='red', label=f'Test loss for a= {cvar_alpha_list[1]}')  # # cvar_alpha_list[1] = a = 0.1\n",
    "# plt.savefig(f'Global t for a = [{cvar_alpha_list[0]},{cvar_alpha_list[1]}].pdf')\n",
    "plt.title(f'Test Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Plot the decision boundaries\n",
    "plt.figure()\n",
    "plt.contourf(xx[0], yy[0], Z[0], alpha=0.4)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=20, edgecolor='k')\n",
    "plt.title(f'Decision boundary for a= {cvar_alpha_list[0]} ')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "# plt.savefig(f'Decision boundary for a = [{cvar_alpha_list[0]}].pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(xx[1], yy[1], Z[1], alpha=0.4)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=20, edgecolor='k')\n",
    "plt.title(f'Decision boundary for a= {cvar_alpha_list[1]} ')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "# plt.savefig(f'Decision boundary for a = [{cvar_alpha_list[1]}].pdf')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t_list_track[0],color='green', label=f'Decision boundary for a= {cvar_alpha_list[0]}')\n",
    "plt.plot(t_list_track[1],color='red', label=f'Decision boundary for a= {cvar_alpha_list[1]}')\n",
    "# plt.savefig(f'Global t for a = [{cvar_alpha_list[0]},{cvar_alpha_list[1]}].pdf')\n",
    "plt.title(f't_final ')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
